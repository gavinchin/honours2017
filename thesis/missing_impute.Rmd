---
title: "Imputation of Missing Values in the Melbourne CBD Pedestrian Dataset"
output:
  html_document:
    theme: united
  html_notebook: default
  pdf_document: default
---
```{r, include=FALSE}
library(plyr)
library(tidyverse)
library(lubridate)
library(magrittr)
library(viridis)
library(gridExtra)
set.seed(12345)
```


```{r data_read, message=F, warning=F, include=FALSE, results='hide', cache=TRUE}
## devtools::install_github("earowang/rwalkr")
# library(rwalkr)
# 
# ped_2014 <- run_melb(year = 2014)
# ped_2015 <- run_melb(year = 2015)
# ped_2016 <- run_melb(year = 2016)
# ped_2017 <- run_melb(year = 2017)
# 
# ped_data <- bind_rows(ped_2014, ped_2015, ped_2016, ped_2017)

# ped_data <- walk_melb(from = "2014-01-01")
ped_data <- read_csv("../data/ped_df.csv")
ped_data$X1 <- NULL
ped_data$Sensor_ID <- ped_data$Sensor
ped_data$Hourly_Counts <- ped_data$Count
ped_data$Date_Time <- ymd_h(paste(ped_data$Date, ped_data$Time, sep = " "))
ped_data$Day <- wday(ped_data$Date_Time, label = T, abbr = F)
ped_data$Month <- month(ped_data$Date_Time, label = T, abbr = F)
ped_data$Time <- as.factor(ped_data$Time)

# for offline use
# ped_dat <- read_csv(file = "../data/ped_df_2.csv")
# ped_data <- ped_dat %>%
#  mutate(
#    Date_Time = as.POSIXct(strptime(Date_Time, format = "%d-%B-%Y %H:%M"),
#      tz = ""),
#    ID = NULL,
#    Mdate = as.integer(Mdate),
#    Hourly_Counts = as.integer(Hourly_Counts),
#    Sensor_ID = as.integer(Sensor_ID),
#    Time = as.integer(Time),
#    Year = as.integer(Year)
#  ) %>% as.data.frame() %>%
#  filter(Date_Time > "2013-12-31 23:00:01") %>%
#  arrange(Date_Time)

# rm(ped_dat)

# ped_loc <- read_csv("../data/Pedestrian_sensor_locations.csv")
```

```{r missing_explicit, echo = F}
## making missings explicit
# dates11 <- full_seq(ped_data$Date_Time, period = 3600)
# id_seq <- rep(1:43, no_T)
# 
# ped_full_seq <- data.frame(Date_Time = dates11, Sensor_ID = id_seq)
# 
# ped_full <- merge(ped_data, ped_full_seq, all.y = T)
# 
# unique(ped_data$Date_Time) %>% length()
# 
#   df_t <- list()
#   no_T <- nrow(as.data.frame(dates11))
#     for (i in 1:43) {
#     i_id <- rep(i, no_T)
#     df_t[[i]] <- data.frame(Date_Time = dates11, Sensor_ID = i_id)
#     }
#   
#   
#   
#   ped_full <- list()
#   ped_list_d <- ped_data %>% split(.$Sensor_ID)
#     for (i in 1:43) {
#     df <- ped_list_d[[i]]
#     ped_full[[i]] <- right_join(df, df_t[[i]], by = "Date_Time")
#     }
#   ped_data <- bind_rows(ped_full) %>% arrange(Date_Time) %>%
#     mutate(Mdate = mday(Date_Time),
#            Day = wday(Date_Time, label = T, abbr = F),
#            Time = hour(Date_Time),
#            Year = year(Date_Time),
#            Month = month(Date_Time, label = T, abbr = F),
#            ID = NULL,
#            Sensor_Name = NULL)
#   
#   ped_names <- ped_loc %>% select(`Sensor ID`, `Sensor Description`)
#   colnames(ped_names) <- c("Sensor_ID", "Sensor_Name")
#   
#   ped_data <- ped_data %>% left_join(ped_names)
#   
#   ped_a <- ped_data %>% split(.$Sensor_ID)
#   ped_n <- numeric()
#   for (i in 1:43) {
#     ped_n[i] <- nrow(df_t[[i]])
#   }
#   
#   sum(ped_n)
#   
#   ad <- setdiff(as.data.frame(ped_a[[1]]$Date_Time), as.data.frame(dates1))
```

## Public Holiday Code

```{r pubhdays, message=F, warning=F, cache=TRUE}
## read data from data.gov.au
pub_hday14 <- read.csv("http://data.gov.au/dataset/b1bc6077-dadd-4f61-9f8c-002ab2cdff10/resource/56a5ee91-8e94-416e-81f7-3fe626958f7e/download/australianpublicholidays-201415.csv---australianpublicholidays.csv.csv")
pub_hday15 <- read.csv("http://data.gov.au/dataset/b1bc6077-dadd-4f61-9f8c-002ab2cdff10/resource/13ca6df3-f6c9-42a1-bb20-6e2c12fe9d94/download/australianpublicholidays-201516.csv")
pub_hday16 <- read.csv("http://data.gov.au/dataset/b1bc6077-dadd-4f61-9f8c-002ab2cdff10/resource/a24ecaf2-044a-4e66-989c-eacc81ded62f/download/australianpublicholidays-201617.csv")
pub_hdays <- rbind(pub_hday14, pub_hday15, pub_hday16)

pub_hdays$Date <- ymd(pub_hdays$Date)
pub_hdays$Month <- pub_hdays$Date %>% month(label = TRUE, abbr = FALSE)
pub_hdays$VIC <- 0
pub_hdays$VIC[grep(glob2rx("*VIC*"), pub_hdays$`Applicable To`)] <- 1
pub_hdays$VIC[grep("NAT", pub_hdays$`Applicable To`)] <- 1

dates1 <- full_seq(ped_data$Date_Time, period = 3600)
hdayvals <- rep(0, length(dates1))
hdayvals[as.Date(dates1) %in% pub_hdays$Date[pub_hdays$VIC == 1]] <- 1
dates2 <- as.Date(rep(dates1, each = 43))
hdayvals2 <- as.numeric(rep(hdayvals, each = 43))

ped_data$IsHDay <- hdayvals2
ped_data$HDay <- ped_data$Day
levels(ped_data$HDay) <- c(levels(ped_data$Day), "Holiday")
ped_data$HDay[ped_data$IsHDay == 1] <- "Holiday"
```
This code uses official public holiday data from `data.gov.au` to check if dates fall on public holidays. The code ensures only public holidays which apply to Victoria are used.

## Code for Day Type
```{r daytypecode, results='hide', cache=TRUE, echo=FALSE}
ped_data$DayType <- ifelse(ped_data$Day == "Sunday", "Sunday",
                  ifelse(ped_data$Day == "Saturday", "Saturday",
                  ifelse(ped_data$Day == "Monday", "Monday",
                  ifelse(ped_data$Day == "Friday", "Friday",
                                                 "Midweek"))))
ped_data$DayType[ped_data$IsHDay == 1] <- "Holiday"
```
Compacts the `HDay` factor into 6 levels, rather than 8 levels, by coding Tuesday, Wednesday and Thursday as factor `"Midweek"`. This is because at most locations, these days have similar patterns. For instance, we see this at Melbourne Central:

```{r, message = F, warning = F, fig.align= 'center', cache=TRUE, echo=FALSE}
ggplot(filter(ped_data, Sensor == "Melbourne Central")) + 
  geom_smooth(aes(x = as.integer(Time), y = Hourly_Counts, colour = Day), se = FALSE) +
  xlab("Time of Day") + ylab("Hourly Count") + theme_minimal() +
  ggtitle("Average pedestrian count at Melbourne Central by day of the week")
```

## GLM model for imputing missings
```{r glm_models, results='hide', cache=TRUE}
ped_list <- ped_data %>% filter(Date < "2017-01-01") %>%
            split(.$Sensor_ID)

ped_coeffs <- list()
ped_model_fitted <- list()
ped_model_summary <- list()
p <- progress_estimated(43)
options(na.action = 'na.omit')
for (i in 1:43) {
  p$tick()$print()
  train <- ped_list[[i]]
  ped_model_i <- glm(Hourly_Counts ~ Month + DayType*Time,
                     family = quasipoisson(), data = train)
  ped_coeffs[[i]] <- ped_model_i$coefficients
  ped_model_summary[[i]] <- summary(ped_model_i)
}
rm(ped_model_i)

```
The code above fits a GLM model for each sensor location. This model uses a quasi-poisson distribution to allow of overdispersion. Using a standard poisson distribution would give the restriction of mean = variance.  
The model is specified as `Hourly_Counts ~ Month + DayType * Time`, where `Month` is given an additive effect, and `DayType * Time` gives an interaction term between `DayType` and `Time`, as well as any other indirect additive effects.
To improve efficiency, only the coefficients of the model are retained, as well as model summarries for model evaluation.  
We train the model only on the data from 2014 to 2016. The test data set is the data available from 2017.

### Test data fit
```{r, results='hide', cache=TRUE}
test_ped <- ped_data %>% filter(Date > "2016-12-31") %>%
            split(.$Sensor_ID)
## required for model.matrix to allow NAs
options(na.action = 'na.pass')

for (i in 1:43) {
  test_df <- model.matrix(Hourly_Counts ~ Month + DayType*Time,
                          data = test_ped[[i]])
  test_coeff <- ped_coeffs[[i]]
  if (length(test_coeff) < ncol(test_df)) {
    missing_cols <- setdiff(colnames(test_df), rownames(as.data.frame(test_coeff)))
    test_df <- test_df[, !(colnames(test_df) %in% missing_cols)]
  }
  
  test_preds <- exp(test_df %*% test_coeff)
  test_ped[[i]]$Fitted <- test_preds
}

test_fitted <- bind_rows(test_ped)

```
Using the coefficients saved for each location, we can easily fit the test data.  
With the function `model.matrix()`, `R` will generate a matrix, $X_j = \{ X_{1j}, X_{2j}, \dots , X_{Tj} \}$, with the factor levels converted into dummy variables for use with the GLM. Then, we can obtain our estimates of $\text{Hourly_Counts}_{j}$, $\widehat{\text{Hourly} \_ \text{Counts}}_{j} = \exp \left( X_j\hat{\beta_j} \right)$ where $\hat{\beta_j}$ is a column vector of the GLM estimated coefficients at the $j^{th}$ sensor location.

# Goodness of Fit
### Visually
```{r, fig.align= 'center', fig.height=24, fig.width=10, fig.retina=1, cache=TRUE, echo=FALSE}
ggplot(filter(test_fitted, Date > "2017-07-23")) +
  geom_line(aes(x = Date_Time, y = Hourly_Counts)) +
  geom_line(aes(x = Date_Time, y = Fitted),
            colour = "violetred2", alpha = 0.75) +
                         facet_wrap(~ Sensor, scales = "free_y", ncol = 4) + theme_minimal() + 
                    theme(axis.text.x = element_text(size = 3),
                          strip.text = element_text(size = 10),
                          axis.text.y = element_text(size = 0)) +
                    xlab("Date") + ylab("Hourly Counts")
```
Looking at plots of actuals (black) against fitted (red) of data from 24 July 2017 to 6 August 2017, we see that some sensors appear to be not operational. We see that there is a mix of very good fitting models and not so good ones. An example of of a reasonably good fitting model is the _Collins Place (North)_ sensor location:
```{r, fig.align= 'center', fig.retina = 1, cache=TRUE}
ggplot(test_fitted %>%
         filter(Date > "2017-07-23",
                Sensor == "Collins Place (North)")) +
      geom_line(aes(x = Date_Time, y = Hourly_Counts)) +
      geom_line(aes(x = Date_Time, y = Fitted), colour = 'violetred2', alpha = 0.7) +
      theme_minimal()
```


```{r, cache=TRUE}
col_pl_test_rmse <- test_fitted %>% filter(Date > "2017-07-23", Sensor == "Collins Place (North)") %>%
               mutate(residsq = (Fitted - Hourly_Counts)^2) %>% .$residsq %>% mean() %>% sqrt()
```

Using RMSE (root mean square error) as a goodness of fit measure, we find that the RMSE of the prediction represented in the plot above is `r round(col_pl_test_rmse, 4)`. In comparison, the model predicted for the sensor located on _Grattan St-Swanston St (West)_ fits very poorly:

```{r, echo=FALSE, fig.align='center', fig.retina=1, cache=TRUE}
ggplot(test_fitted %>%
         filter(Date > "2017-07-23",
                Sensor == "Grattan St-Swanston St (West)")) +
      geom_line(aes(x = Date_Time, y = Hourly_Counts)) +
      geom_line(aes(x = Date_Time, y = Fitted), colour = 'violetred2', alpha = 0.7) +
      theme_minimal()
```


```{r, echo=FALSE, cache=TRUE}
gratt_test_rmse <- test_fitted %>% filter(Date > "2017-07-23", Sensor == "Grattan St-Swanston St (West)") %>%
               mutate(residsq = (Fitted - Hourly_Counts)^2) %>% .$residsq %>% mean() %>% sqrt()

gratt_test_rmse_july <- test_fitted %>% filter(Date > "2017-07-23", Date < "2017-08-01", Sensor == "Grattan St-Swanston St (West)") %>%
               mutate(residsq = (Fitted - Hourly_Counts)^2) %>% .$residsq %>% mean() %>% sqrt()

gratt_test_rmse_aug <- test_fitted %>% filter(Date > "2017-07-31", Sensor == "Grattan St-Swanston St (West)") %>%
               mutate(residsq = (Fitted - Hourly_Counts)^2) %>% .$residsq %>% mean() %>% sqrt()
```

Here, the poor fit results in a RMSE of `r round(gratt_test_rmse, 4)` for the period between Jul 24 to Aug 6. It appears as though the additive effect of `Month` is exaggerated, where we see that predictions in July are significantly underestimated, compared to August predictions which have adequate fit.
If we compare the RMSE between July predictions and August predictions, we find that the RMSE of the predictions of Aug 1 to Aug 6 is much lower than that of the predictions for Jul 24 to Jul 31 (`r round(gratt_test_rmse_aug, 4)` compared to `r round(gratt_test_rmse_july, 4)`)

```{r, fig.align= 'center', fig.retina=1, echo=FALSE, cache=TRUE, warning=F}
ggplot(filter(test_fitted, Sensor_ID == "Grattan St-Swanston St (West)")) +
  geom_path(aes(x = Time, y = Hourly_Counts, group = Date), alpha = 0.1) +
  geom_path(aes(x = Time, y = Fitted, group = Date), colour = "violetred2", alpha = 0.1) +
  facet_wrap(~ DayType) +
      theme_minimal()
```

Looking at the counts for each day, we see that the variance during weekdays is quite large. This would suggest that the day of the week and time of the day do not explain a lot of the variance in the data.

```{r, fig.align= 'center', fig.retina=1, echo=FALSE, cache=TRUE, warning = F}
ggplot(filter(ped_data, Sensor_ID == "Grattan St-Swanston St (West)")) +
  geom_path(aes(x = Time, y = Hourly_Counts, group = Date, colour = Month), alpha = 0.1) +
  facet_wrap(~ DayType) +
      theme_minimal()

ggplot(filter(ped_data, Sensor_ID == "Grattan St-Swanston St (West)")) + geom_line(aes(x = Date_Time, y = Hourly_Counts)) +
      theme_minimal()
```

However, we see that the variance within a month is quite low comparatively. It is worth noting that the data available at this location is very poor.  
Plotting all available observations, we see we only have data starting from `r head(na.omit(ped_data[ped_data$Sensor_ID == "Grattan St-Swanston St (West)", ])$Date, 1)`.

