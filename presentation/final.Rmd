---
title: "Modelling the City of Melbourne Pedestrian Data"
subtitle: "Thesis Presentation"
author: "Gavin Chin, supervised by Di Cook"
date: "2 October 2017"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: ["default", "myremark.css"]
    self_contained: false
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE, echo = FALSE, warning=FALSE, message=FALSE}
options(htmltools.dir.version = FALSE,
        scipen = 100, cache = TRUE)
library(tidyverse)
library(ggmap)
library(knitr)
library(lubridate)
library(gridExtra)
library(viridis)
library(pander)
library(png)
library(grid)
set.seed(13131313)
## quicker data source compared to rwalkr::walk_melb() 
## ped_data <- read_csv("http://209.148.91.227/files/ped_df_28sep17.csv")

  ped_data <- read_csv("../data/ped_df.csv")
      
  ped_loc <- read_csv("../data/Pedestrian_sensor_locations.csv")
  
  melb <- get_map(location=c(mean(range(ped_loc$Longitude)),
                               mean(range(ped_loc$Latitude))),
                                zoom=14)
```

class: center

# Introduction

The City of Melbourne has an Open Data Platform which makes council data available to the public  

This project will focus on the pedestrian sensor data

```{r, echo = FALSE, fig.height=5, fig.retina = 3}
ggmap(melb) + geom_point(aes(x = Longitude, y = Latitude),
                         data = ped_loc, colour = "red",
                         size = 4, alpha = 0.7, shape = 17) +
              labs(title = "Melbourne Pedestrian Sensor Locations") +
              xlab(NULL) + ylab(NULL) +
              theme(plot.title = element_text(hjust = 0.5),
                    plot.caption = element_text(hjust = 0.5),
                    axis.text.x = element_blank(),
                    axis.text.y = element_blank(),
                    axis.ticks = element_blank())
```


---
class: middle

## How is the data collected?

The City of Melbourne has `r nrow(ped_loc)` sensors installed around Melbourne's CBD  
- These sensors are positioned under an awning or street pole, and count the number of people passing each hour

```{r, echo = FALSE, fig.align= 'center', fig.retina = 3, out.height= "369px", out.width = "852px"}
include_graphics("../img/ped_sensors.png")
```

_source: http://www.pedestrian.melbourne.vic.gov.au/_

---
class: middle
## Accessing the data

- The data is available as a `.csv` at https://data.melbourne.vic.gov.au/, and is updated monthly
- Alternatively, the data is also available as daily data from http://www.pedestrian.melbourne.vic.gov.au/
  * The `rwalkr` R package by Earo Wang provides an API to access the data from both sources easily in R in a tidy format
- Both sources of data provide the same counts data, but have different forms

We use the daily data sourced from `pedestrian.melbourne.vic.gov.au` rather than the data available at the Open Data Platform as missing values are explicit
- accessible using `rwalkr::walk_melb()`

---

# Format of the data
#### Variables available:
- Time and date, sensor ID/location and hourly pedestrian count (volume)

```{r, echo=FALSE}
print(ped_data %>% as.tibble, n = 4)
```
```{r, include = FALSE, echo = FALSE, warning=FALSE, message=FALSE, cache = TRUE}
ped_data$Sensor_ID <- ped_data$Sensor
ped_data$Hourly_Counts <- ped_data$Count
ped_data$Date_Time <- ymd_h(paste(ped_data$Date, ped_data$Time, sep = " "))
ped_data$Day <- lubridate::wday(ped_data$Date_Time, label = TRUE, abbr = F)
ped_data$Month <- lubridate::month(ped_data$Date_Time, label = TRUE, abbr = F)
ped_data$Time <- as.factor(ped_data$Time)
ped_data$WkYr <- paste(week(ped_data$Date_Time), year(ped_data$Date_Time), sep = "-")

## long to wide
ped_data %>% select(Sensor_ID, Count, Date_Time, Time, Month, Day) %>% 
  spread(Sensor_ID, Count) -> dfa

pub_hday14 <- read.csv("http://data.gov.au/dataset/b1bc6077-dadd-4f61-9f8c-002ab2cdff10/resource/56a5ee91-8e94-416e-81f7-3fe626958f7e/download/australianpublicholidays-201415.csv---australianpublicholidays.csv.csv")
pub_hday15 <- read.csv("http://data.gov.au/dataset/b1bc6077-dadd-4f61-9f8c-002ab2cdff10/resource/13ca6df3-f6c9-42a1-bb20-6e2c12fe9d94/download/australianpublicholidays-201516.csv")
pub_hday16 <- read.csv("http://data.gov.au/dataset/b1bc6077-dadd-4f61-9f8c-002ab2cdff10/resource/a24ecaf2-044a-4e66-989c-eacc81ded62f/download/australianpublicholidays-201617.csv")
pub_hdays <- rbind(pub_hday14, pub_hday15, pub_hday16)

pub_hdays$Date <- ymd(pub_hdays$Date)
pub_hdays$Month <- lubridate::month(pub_hdays$Date, label = TRUE, abbr = F)
pub_hdays$VIC <- 0
pub_hdays$VIC[grep(glob2rx("*VIC*"), pub_hdays$`Applicable.To`)] <- 1
pub_hdays$VIC[grep("NAT", pub_hdays$`Applicable.To`)] <- 1


dfa <- dfa %>% mutate(IsHDay = (date(Date_Time) %in% pub_hdays$Date[pub_hdays$VIC == 1]))

dfa$DayType <-       ifelse(dfa$Day == "Sunday", "Sunday",
                     ifelse(dfa$Day == "Saturday", "Saturday",
                    ifelse(dfa$Day == "Monday", "Monday",
                     ifelse(dfa$Day == "Friday", "Friday",
                                  "Midweek"))))
dfa$DayType[dfa$IsHDay == 1] <- "Holiday"
dfa$HDay <- ifelse(dfa$IsHDay == 1, "Holiday", dfa$Day)
dfa <- dfa %>% mutate(WkYr = paste(week(Date_Time), year(Date_Time)), sep = "-")
original_df <- dfa
dfa <- filter(dfa, Date_Time < "2017-01-01")
ori_2 <- filter(original_df, Date_Time < "2017-01-01")
```

```{r, echo=FALSE, fig.retina = 3, fig.height=3, fig.width= 10, warning=F, message = F}
pd2 <- ggplot(dfa %>% 
              filter(Date_Time > "2016-07-12 00:00:00",
                     Date_Time < "2016-07-25 00:00:00")) +
        scale_y_continuous(limits = c(0, 7000)) +
        xlab("Time and Date") +
        theme_minimal()

p1 <-  pd2 + geom_line(aes(x = Date_Time, y = `Flagstaff Station`))
p2 <-  pd2 + geom_line(aes(x = Date_Time, y = `Southbank`))
p3 <-  pd2 + geom_line(aes(x = Date_Time, y = `State Library`))

grid.arrange(p1, p2, p3, ncol = 1)
```

---
class: center
# The Problem: Prediction of pedestrian traffic
```{r, echo=FALSE, fig.retina = 3, fig.height=5, fig.width= 10, warning=F, message = F}
pd <- ggplot(dfa %>% filter(Date_Time < "2017-01-01 00:00:00",
                            Date_Time > "2013-12-31 00:00:00")) +
        scale_y_continuous(limits = c(0, 7000)) +
        xlab(NULL) +
        theme_minimal()

p1 <-  pd + geom_line(aes(x = Date_Time, y = `Flagstaff Station`))
p2 <-  pd + geom_line(aes(x = Date_Time, y = `Southbank`))
p3 <-  pd + geom_line(aes(x = Date_Time, y = `State Library`)) +
            xlab("Time and Date")

grid.arrange(p2, p1, p3, nrow = 3, ncol = 1)
```


- Different locations have different counts and patterns
- We want to predict using 2014-2016 data to train our model

---
## Why do we want to model pedestrian traffic?

### Being able to predict how many people pass through a certain location can be used by the government sector as well as the private sector  

#### Examples ways it can be used: 

- Government example uses:  
  * Infrastructure planning
  * Security planning
- Private example uses:
  * Marketing campaign planning
  * Resource management
  * Investment planning
---
class: middle, center
# Missing Data
```{r, echo = FALSE, fig.retina=2, fig.width= 12, message=FALSE, warning=FALSE, cache = TRUE}
miss_plot_dat <- ped_data %>% filter(Date_Time > "2014-06-01",
                                     Date_Time < "2015-06-01") %>%
  group_by(WkYr, Sensor_ID) %>%
  summarise(Weekly_Count = sum(Hourly_Counts))

miss_plot_dat2 <- ped_data %>% group_by(Date) %>%
  filter(Date > "2014-06-01", Date < "2015-06-01") %>% left_join(miss_plot_dat)

random_sensors <- c("Collins Place (North)","Southbank","Spring St-Lonsdale St (South)",
                    "Flinders St-Spring St (West)","Convention/Exhibition Centre","Sandridge Bridge")

mdf <- miss_plot_dat2 %>% filter(Sensor_ID %in% random_sensors) %>%
        mutate(Sensor_ID = as.factor(Sensor_ID))

ggplot(mdf) +
  geom_point(aes(y = Sensor_ID, x = Date, colour = Weekly_Count), shape = 15, size = 15, alpha = 0.15) +
  ylab("Location") + theme_minimal() + theme(legend.position = "bottom") +
  scale_color_viridis(option = "magma", guide = guide_colorbar(barwidth = 10)) +
  ggtitle("Weekly Counts for at various locations")
  
```

- Some small and large periods of missing data
- We need to impute these values before building a predictive model
---

## Imputation of missing values

- A simple approach of using a single model specification at all locations did not work well
  * Some sensors had large proportions of missing data, particularly pre-2015
- Need to use different model for sensors with large proportion of missing values and those with a small proportion of missing values

```{r model_alg, echo = FALSE, warning=FALSE, message=FALSE, cache = TRUE}
n_obs <- nrow(dfa)
threshold <- 0.1
threshold_miss_hrs <- 17

run_lengths <- list()

### code from:
## http://www.win-vector.com/blog/2014/05/trimming-the-fat-from-glm-models-in-r/
stripGlmLR = function(cm) {
  cm$y = c()
  cm$model = c()
  
  cm$residuals = c()
  cm$fitted.values = c()
  cm$effects = c()
  cm$qr$qr = c()  
  cm$linear.predictors = c()
  cm$weights = c()
  cm$prior.weights = c()
  cm$data = c()
  
  
  cm$family$variance = c()
  cm$family$dev.resids = c()
  cm$family$aic = c()
  cm$family$validmu = c()
  cm$family$simulate = c()
  attr(cm$terms,".Environment") = c()
  attr(cm$formula,".Environment") = c()
  
  cm
}

for(i in unique(ped_data$Sensor)){
    dfa[is.na(dfa[, i]), i] <- 0
    na_length_check <- rle(as.numeric(unlist(dfa[,i])))
    maybeNA <- rep((na_length_check$lengths > threshold_miss_hrs),
                   times = na_length_check$lengths)
    rl <- dfa[, i] %>% unlist %>% rle()
    dfa[maybeNA, i] <- NA
    defsNA <- is.na(ori_2[, i])
    dfa[defsNA, i] <- NA
    run_lengths[[i]] <- rl
}

na_prop <- numeric()

for (i in unique(ped_data$Sensor)){
  na_prop[i] <- sum(is.na(dfa[, i])) / n_obs
}

large_miss_sensors <- names(na_prop[na_prop > threshold])
small_miss_sensors <- setdiff(names(na_prop), large_miss_sensors)

models <- list()
fitted_data <- list()
dfa2 <- dfa
for(i in small_miss_sensors){
  options(na.action = na.omit)
  sensor_df <- unlist(dfa[, i])
  model <- glm(sensor_df ~  Month + DayType*Time,
               family = 'quasipoisson', data = dfa)
  models[[i]] <- (stripGlmLR(model))
}

  options(na.action = na.pass)
for(i in small_miss_sensors){
  ts_dat <- data.frame(Month = dfa$Month, DayType = dfa$DayType, Time = dfa$Time)
  fitted_data[[i]] <- predict.glm(models[[i]],newdata = ts_dat, type = "response")
}
  options(na.action = na.omit)
  for(i in small_miss_sensors){
    dfa2[is.na(dfa[, i]), i] <- fitted_data[[i]][is.na(dfa[, i])]
  }
  
  lats <- ped_loc$Latitude
lons <- ped_loc$Longitude
ids <- ped_loc$`Sensor Description`

## name fix dictionary
{
ids[ids == "Flinders Street Station Underpass"] <- "Flinders St Station Underpass"
ids[ids == "Flinders St-Spark La"] <- "Flinders St-Spark Lane"
ids[ids == "Queen St (West)"] <-  "Queen Street (West)"
ids[ids == "The Arts Centre"] <- "Vic Arts Centre"
ids[ids == "Lonsdale St-Spring St (West)"] <- "Spring St-Lonsdale St (South)"
ids[ids == "Lygon St (East)"] <- "Lygon Street (East)"
ids[ids == "QV Market-Elizabeth St (West)"] <- "QV Market-Elizabeth (West)"
ids[ids == "Melbourne Convention Exhibition Centre"] <- "Convention/Exhibition Centre"
ids[ids == "St Kilda Rd-Alexandra Gardens"] <- "St. Kilda-Alexandra Gardens"
}
loc <- data.frame(lons, lats, ids)
rownames(loc) <- ids

# Calculates the geodesic distance between two points specified by radian latitude/longitude using the
# Spherical Law of Cosines (slc)
# source: https://www.r-bloggers.com/great-circle-distance-calculations-in-r/
gcd.slc <- function(long1, lat1, long2, lat2) {
  R <- 6371 # Earth mean radius [km]
  d <- acos(sin(lat1)*sin(lat2) + cos(lat1)*cos(lat2) * cos(long2-long1)) * R
  return(d) # Distance in km
}

neighbours <- list()

for (i in large_miss_sensors){
  # p$tick()$print()
  sensor_dists <- numeric()
  for (j in small_miss_sensors){
      dists <- gcd.slc(loc[i,]$lons, loc[i,]$lats, loc[j,]$lons, loc[j,]$lats)
      sensor_dists[j] <- dists
  }
  names(sensor_dists) <- small_miss_sensors
  closest_sensor <- sensor_dists %>% sort %>%
                    head(2) %>% names
  close_df_scale <- dfa2[, closest_sensor[1]] %>% unlist %>% scale
  close_df_scale2 <- dfa2[, closest_sensor[2]] %>% unlist %>% scale
  
  neighbours[[i]] <- closest_sensor
  
  sensor_df <- dfa2[, i] %>% unlist
  
  tr_dat <- data.frame(sensor_df, close_df_scale, close_df_scale2, Time = dfa$Time)
  options(na.action = na.omit)
  model <- glm(sensor_df ~ Time*close_df_scale + Time*close_df_scale2,
               data = tr_dat, family = quasipoisson())
  models[[i]] <- stripGlmLR(model)
  
  options(na.action = na.pass)
  fitted_data[[i]] <- predict(model, newdata = tr_dat, type = "response")
  options(na.action = na.omit)

  dfa2[is.na(dfa[, i]), i] <- fitted_data[[i]][is.na(dfa[, i])]
  
}

```

```{r, echo = FALSE, fig.width = 7, fig.height = 4.5, fig.retina = 3, fig.align = 'center'}
fit_dat_plot <- data.frame(Date_Time = dfa2$Date_Time,
                           fit_ = dfa2$`Southern Cross Station`) %>%
                  filter(Date_Time < "2014-07-06 00:00:00",
                         Date_Time > "2014-07-04 12:00:00") %>%
                        .$fit_

ggplot(original_df %>%  filter(Date_Time < "2014-07-06 00:00:00",
                               Date_Time > "2014-07-04 12:00:00")) +
  geom_line(aes(x = Date_Time, y = fit_dat_plot),
            colour = plasma(1, begin = .75, end = 0),
            size = 2) +
  geom_line(aes(x = Date_Time, y = `Southern Cross Station`),
            size  = 2) +
  theme_minimal() + xlab("Time") +
  ylab("Pedestrian Counts") +
  ggtitle("Pedestrian Counts at Southern Cross Station on 04/07/2014 - 05/07/2014")
```

---
class: middle
# Imputation Algorithm

#### Needed to develop an algorithm to:

- **Step 1:** Find which sensors have a "large" proportion of missing values  
- **Step 2:** At each sensor location, run appropriate GLM model with quasipoisson error distribution for imputation  
  * if "small", use GLM with `Hourly_Counts` as response variable and time and date based variables as predictors
  * if "large", use GLM with `Hourly_Counts` as response variable and neighbouring sensor counts as predictors
- **Step 3:** Replace missing values with imputed values  

---
## Step 1: Calculating proportion of missing values
- Need to first fix potential false zero-count values
  * Some observations of `Count = 0` may actually be missing values
  * Include a check for long periods with zero count, and replace with `NA` if too long
- Classify sensors as "small" or "large" proportion of missing values with a threshold proportion
  * We used 10% as the threshold proportion

```{r, echo = FALSE, fig.retina = 3, fig.height = 3, fig.align='center'}
ggplot() + geom_histogram(aes(x = na_prop*100), bins = 12) +
          xlab("Percentage of Missing Values (%)") +
          ylab("Number of Sensors") + theme_minimal() +
          geom_vline(xintercept = 10, size = 2,
                     colour = 'red') +
          geom_text(aes(label = "10% Threshold", x = 10, y = 1),
                    colour = "white") +
          ggtitle("Histogram of Proportions of Missing Values during 2014 - 2016")
```


---
class: middle, center
## Step 1: Classification of sensors by "small" and "large" proportion of  missing values

```{r, fig.retina = 4, echo = FALSE, message = FALSE, fig.width = 11, fig.align = 'center', warning=FALSE}
loc <- loc %>% mutate(cl = ifelse((loc$ids %in% small_miss_sensors),
                                  "Small", "Large"))
melb2 <- get_map(location = c(mean(range(ped_loc$Longitude)),
                            mean(range(ped_loc$Latitude))),
                            zoom=14, maptype = "satellite")
perf_loc <- c(144.937, -37.826, 144.977, -37.794)

ggmap(melb2) + geom_point(aes(x = lons, y = lats, colour = cl),
                         data = loc, alpha = .6, size = 8, shape = 15) +
              labs(title = "Melbourne Pedestrian Sensor Locations") +
              xlab(NULL) + ylab(NULL) +
              scale_color_viridis(name = "Proportion of missing values",
                                  discrete = TRUE, option = "C") +
              scale_x_continuous(limits = c(144.937, 144.977)) +
              scale_y_continuous(limits = c(-37.83, -37.79)) +
              theme(plot.title = element_text(hjust = 0.5),
                    plot.caption = element_text(hjust = 0.5),
                    axis.text.x = element_blank(),
                    axis.text.y = element_blank(),
                    axis.ticks = element_blank(),
                    legend.position = "right")
```

---

## Step 2a: Fit models at sensors with "small" proportion of missing values

At each sensor location with small proportion of missing values, we estimate a generalised linear model with:
$$\mu_{\text{Sensor}, \text{Month}, \text{DayType}, \text{Time}} \sim \text{Month} + \text{Time}\times \text{DayType}$$
and a quaispoisson error distribution.
- Quasipoisson regression allows for $Var[\text{HourlyCount}] = \theta \mu$ (ie. overdispersion)
- These models only use time and date based variables to predict pedestrian counts
  * `DayType` is a factor level/categorical variable based on the day of the week and public holidays
  * `Time` is treated as a factor level, as it has a non-linear relationship with counts

---

## Step 2b: Replacing missing values with imputed values

- Need to fill the missing data at the sensors with small proportions of missing values first
  * The imputation models at sensors with large proportions of missing values are based on neighbouring sensors
  * Requires complete data at neighbours to work properly

```{r, echo=FALSE, fig.height=4, fig.retina=3, fig.width=10, message=FALSE, warning=FALSE}
fit_dat_plot <- data.frame(Date_Time = dfa2$Date_Time,
                           fit_ = dfa2$`Flagstaff Station`) %>%
                  filter(Date_Time < "2014-07-05 15:00:00",
                         Date_Time > "2014-07-03 23:00:00") %>%
                        .$fit_

p1 <-  ggplot(original_df %>%  filter(Date_Time < "2014-07-05 15:00:00",
                                 Date_Time > "2014-07-03 23:00:00")) +
        geom_line(aes(x = Date_Time, y = `Flagstaff Station`),
                  size  = 1.5) +
        geom_point(aes(x = Date_Time, y = `Flagstaff Station`),
                  size  = 2) +
        theme_minimal() + xlab("Time") +
        ylab("Pedestrian Counts") +
        ggtitle("Flagstaff Station")
p2 <-  ggplot(original_df %>%  filter(Date_Time < "2014-07-05 15:00:00",
                                 Date_Time > "2014-07-03 23:00:00")) +
        geom_line(aes(x = Date_Time, y = fit_dat_plot),
                  colour = plasma(1, begin = .375, end = 0),
                  size = 1.5) +
        geom_line(aes(x = Date_Time, y = `Flagstaff Station`),
                  size  = 1.5) +
        geom_point(aes(x = Date_Time, y = `Flagstaff Station`),
                  size  = 2) +
        theme_minimal() + xlab("Time") +
        ylab("Pedestrian Counts") +
        ggtitle("Flagstaff Station with imputed values")
grid.arrange(p1, p2, nrow = 1)
```


---
class: middle
## Step 2c: Find neighbours for sensors with "large" proportion of missing values

```{r, echo = FALSE, fig.retina = 6, message=FALSE, warning=F, fig.height = 4, fig.align='center'}
rownames(loc) <- loc$ids
melb3 <- get_map(location = c(loc["Southbank",]$lons,
                              loc["Southbank",]$lats),
                 zoom = 17, maptype = "roadmap")

sens_lines_lon <- as.numeric(c(loc["Flinders St Station Underpass",]$lons,
                loc["Southbank",]$lons,
                loc["Sandridge Bridge",]$lons))

sens_lines_lat <- as.numeric(c(loc["Flinders St Station Underpass",]$lats,
                loc["Southbank",]$lats,
                loc["Sandridge Bridge",]$lats))

sens_lines <- data.frame(lon = sens_lines_lon,
                         lat = sens_lines_lat)
ggmap(melb3) + geom_line(aes(x = lon, y = lat),
                        data = sens_lines, size = 2.5,
                        colour = viridis(3)[2], alpha = 0.5) +
               geom_point(aes(x = lons, y = lats, colour = cl),
                          size = 8,
                          data = loc) +
               scale_colour_viridis(discrete = TRUE,
                                    name = "Sensor NA Prop.",
                                    begin = 0.5, end = 0) +
               geom_point(aes(x = loc["Southbank",]$lons,
                              y = loc["Southbank",]$lats),
                          size = 16, colour = viridis(3)[2]) +
               geom_text(aes(label = ids, x = lons, y = lats),
                         data = loc, size = 4,
                         nudge_y = -0.0002) +
              xlab(NULL) + ylab(NULL) +
              theme(plot.title = element_text(hjust = 0.5),
                    plot.caption = element_text(hjust = 0.5),
                    axis.text.x = element_blank(),
                    axis.text.y = element_blank(),
                    axis.ticks = element_blank(),
                    legend.position = c(.8,0.2))

```
- Need to use data from neighbouring sensors
- Neighbour defined as geographical neighbours for simplicity
  * Find two closest sensors by great-circle (haversine) distance

---
class: center, middle
## _Step 2c: Fit find neighbours sensors with "large" proportion of missing values_

```{r, echo = FALSE, fig.retina = 6, message=FALSE, warning=F, include=FALSE}
i <- "Southbank"
  sensor_dists <- numeric()
  for (j in small_miss_sensors){
      dists <- gcd.slc(loc[i,]$lons, loc[i,]$lats, loc[j,]$lons, loc[j,]$lats)
      sensor_dists[j] <- dists
  }
  names(sensor_dists) <- small_miss_sensors
  closest_sensor <- sensor_dists %>% sort %>%
                    head(2) %>% names
  dfa3 <- original_df %>% filter(Date_Time > "2017-01-01")
  close_df <- dfa3[, closest_sensor[1]] %>% unlist
  close_df2 <- dfa3[, closest_sensor[2]] %>% unlist
  
  close_dfsc <- scale(close_df)
  close_dfsc2 <- scale(close_df2)

  
  sensor_df <- dfa3[, i] %>% unlist
  sensor_dfsc <- scale(sensor_df)
  
  tr_dat <- data.frame(sensor_df, close_df, close_df2,
                       Time = as.integer(dfa3$Time),
                       Date_Time = dfa3$Date_Time,
                       HDay = dfa3$HDay) %>%
            gather(key = Sensor, value = Count, sensor_df:close_df2)
  
  tr_dat_sc <- data.frame(sensor_dfsc, close_dfsc, close_dfsc2,
                       Time = as.integer(dfa3$Time),
                       Date_Time = dfa3$Date_Time,
                       HDay = dfa3$HDay) %>%
            gather(key = Sensor, value = Count, sensor_dfsc:close_dfsc2)
  
p1 <- ggplot(tr_dat %>% filter(Date_Time < "2017-03-02",
                         Date_Time > "2017-02-26")) +
      geom_line(aes(x = Date_Time, y = Count,
                    colour = Sensor), size = 2) +
      scale_colour_viridis(discrete = TRUE,
                           labels = c("Neighbour 1 (Flinders St Station Underpass)",
                                      "Neighbour 2 (Sandridge Bridge)",
                                      "Target (Southbank)")) +
      xlab("Time") + theme_minimal() + ggtitle("Unscaled Neighbours") +
      theme(legend.position = "none")
      
p2 <- ggplot(tr_dat_sc %>% filter(Date_Time < "2017-03-02",
                         Date_Time > "2017-02-26")) +
      geom_line(aes(x = Date_Time, y = Count,
                    colour = Sensor), size = 2) +
      scale_colour_viridis(discrete = TRUE,
                           labels = c("Neighbour 1 (Flinders St Station Underpass)",
                                      "Neighbour 2 (Sandridge Bridge)",
                                      "Target (Southbank)")) +
      xlab("Time") + ggtitle("Scaled/Stadardised Neighbours") +
      ylab("Standard Deviation") + theme_minimal() +
      theme(legend.position = "bottom")
```


```{r, echo = FALSE, fig.retina = 6, message=FALSE, warning=F, fig.width=10, fig.height=6}
grid.arrange(p1, p2, ncol = 1)
```

- Need to standardise counts at neighbours to match patterns

---
class: middle
## _Step 2c: Fit models using neighbours_
At each sensor location with "large" proportion of missing values, we estimate a generalised linear model with:  

$$\mu_{\text{Time}, \text{HourlyCounts}_{\text{neighbours}}^{SC}} \sim \text{Time} \times \text{HourlyCounts}_{\text{neighbour 1}}^{SC} + \text{Time} \times \text{HourlyCounts}_{\text{neighbour 2}}^{SC}$$

and a quaispoisson error distribution.  

- use interaction with `Time`, the hour of the day, and the scaled counts at neighbours
  * Again, `Time` is treated as a factor variable
  * Counts scaled by standardising each sensor to $\mu = 0$ and $\sigma^2 = 1$

---
## Step 3: Replacing missing values with imputed values

Using the models estimated, we can impute the missing values
- For sensors with large proportion of missing values, we needed to have complete data at the neighbours
  * Can't use neighbours with large proportion of missing values
  * Use imputed data from sensors with small proportion of missing values

```{r, echo = FALSE, fig.retina = 6, message=FALSE, warning=F, fig.width=10, fig.height = 4}
p1 <-   ggplot(original_df %>% filter(Date_Time < "2017-01-01")) +
        geom_line(aes(x = Date_Time, y = `Southbank`)) +
        scale_y_continuous(limits = c(0, 7500)) +
        theme_minimal() + ylab("Pedestrian Count") +
        ggtitle("Southbank Pedestrian Counts before Imputation")

p2 <-   ggplot(original_df %>% filter(Date_Time < "2017-01-01")) +
        geom_line(aes(x = Date_Time, y = dfa2$Southbank),
                  colour = viridis(3)[1]) +
        geom_line(aes(x = Date_Time, y = `Southbank`)) +
        scale_y_continuous(limits = c(0, 7500)) +
        theme_minimal() + ylab("Pedestrian Count") +
        ggtitle("Southbank Pedestrian Counts after Imputation")
grid.arrange(p1, p2, ncol = 1)
```

---
class: middle
# Implementation of the imputation algorithm

- general code which will run the algorithm
- inputs required: 
  * data in wide format
  * threshold value for missing values proportion (_default:_ `threshold = 0.1`)
  * max length of repeated values (_default:_ `maxlength = 17`)
- use R and RStudio with packages:
  * `dplyr`, `lubridate` and `tidyr` for data manipulation
  * `foreach` and `doSNOW` for parallel computing
  * `rwalkr` for accessing latest data
---
class: center
## Imputation Model Evaluation
### In-sample fit

```{r, echo = FALSE, fig.retina = 6, message=FALSE, warning=F, fig.width = 9}
p1 <-   ggplot(data = NULL, aes(x = original_df %>% 
             filter(Date_Time < "2017-01-01") %>%
             .[, "Southern Cross Station"],
           y = fitted_data$`Southern Cross Station`)) +
           geom_point(alpha = 0.05) +
           geom_abline(aes(slope = 1, intercept = 0),
                       colour = viridis(3)[1],
                       alpha = 0.5, size = 1.5,
                       linetype = 2) +
           geom_smooth(method = "lm", size = 1.5, alpha = 0.7, 
                       colour = viridis(3)[3]) +
           theme_minimal() + theme(aspect.ratio = 1) +
           scale_x_continuous(limits = c(0, 4000)) +
           scale_y_continuous(limits = c(0, 4000)) +
           xlab("Actual Pedestrian Count") +
           ylab("Predicted Pedestrian Count") +
           ggtitle("Actual against Predicted at Southern Cross Station")

p2 <-   ggplot(data = NULL, aes(x = dfa %>% 
             filter(Date_Time < "2017-01-01") %>%
             .[, "Southbank"],
           y = fitted_data$`Southbank`)) +
           geom_point(alpha = 0.05) +
           geom_abline(aes(slope = 1, intercept = 0),
                       colour = viridis(3)[1],
                       alpha = 0.5, size = 1.5,
                       linetype = 2) +
           geom_smooth(method = "lm", size = 1.5, alpha = 0.7, 
                       colour = viridis(3)[3]) +
           theme_minimal() + theme(aspect.ratio = 1) +
           scale_x_continuous(limits = c(0, 6000)) +
           scale_y_continuous(limits = c(0, 6000)) +
           xlab("Actual Pedestrian Count") +
           ylab("Predicted Pedestrian Count") +
           ggtitle("Actual against Predicted at Southbank")
grid.arrange(p1, p2, nrow = 1)
```

```{r}
MARE_glm_tr <- numeric()
for (i in unique(ped_data$Sensor)){
  MARE_glm_tr[i] <- mean(abs(fitted_data[[i]] - unlist(dfa[, i])), na.rm = T) / mean(unlist(dfa[, i]), na.rm = T)
}
```

---
class: center, middle
## Imputation Model Evaluation
### Out-of-sample cross validation
```{r, echo = FALSE}
test_fit <- list()
test_df <- original_df %>% filter(Date_Time > "2016-12-31")
  for(i in small_miss_sensors){
    test_fit[[i]] <- predict.glm(models[[i]],
                                 newdata = test_df,
                                 type = "response")
    }

  for(i in large_miss_sensors){
    ts_data <- data.frame(close_dfsc = (test_df[, neighbours[[i]][1]] %>% scale),
                          close_dfsc2 = (test_df[, neighbours[[i]][2]] %>% scale),
                          Time = test_df$Time)
    colnames(ts_data) <- c("close_df_scale", "close_df_scale2", "Time")
    test_fit[[i]] <- predict.glm(models[[i]],
                                 newdata = ts_data,
                                 type = "response")
  }
MARE_glm_ts <- numeric()
  for (i in unique(ped_data$Sensor)){
    MARE_glm_ts[i] <- mean(abs(test_fit[[i]] - unlist(test_df[, i])),
                           na.rm = T) / mean(unlist(test_df[, i]), na.rm = T)
    }

```

```{r, echo = FALSE, fig.retina = 6, message=FALSE, warning=F, fig.width = 4.5, fig.height = 3.5}
ggplot(test_df, aes(x = `Bourke Street Mall (North)`,
                    y = test_fit$`Bourke Street Mall (North)`)) +
                  geom_point(alpha = 0.5) +
                  scale_x_continuous(limits = c(0, 5000)) +
                  scale_y_continuous(limits = c(0, 5000)) +
                  xlab("Actual") + ylab("Fitted") + theme(aspect.ratio = 1) +
                  geom_abline(aes(slope = 1, intercept = 0),
                              colour = viridis(3)[1],
                              alpha = 0.7, size = 1.5,
                              linetype = 2) +
                  geom_smooth(method = "lm", size = 1.5, 
                       colour = viridis(3)[3]) +
                  ggtitle("Actual vs Fitted \n at Bourke Street Mall (North) in 2017")

```

```{r, echo = FALSE, fig.retina = 6, message=FALSE, warning=F, fig.width = 10, fig.height = 3}
fit_t_plot <- data.frame(fit = test_fit$`Bourke Street Mall (North)`,
                         Date_Time = test_df$Date_Time)  
ggplot(test_df %>% filter(Date_Time > "2017-08-31")) +
  geom_line(aes(x = Date_Time,
                y = fit), data = (fit_t_plot %>%
                                  filter(Date_Time > "2017-08-31")),
            colour = magma(3)[2], alpha = 0.6) +
  geom_line(aes(x = Date_Time,
                y = `Bourke Street Mall (North)`),
            alpha = 0.6) +
  scale_y_continuous(limits = c(0, 5000)) +
  theme_minimal() + xlab("Time") + ylab("Count") +
  ggtitle("Bourke Street Mall (North)")

```

---

# Predictive model using imputed data
- can perform analysis at all locations now with complete data
```{r, echo = FALSE, fig.retina = 6, message=FALSE, warning=F, fig.width = 10, fig.height = 3}
melb_cent_mon9 <- dfa2 %>%
  filter(Day == "Monday",
         Time == 9) %>%
         .$`Melbourne Central`

monday_hdays <- dfa2 %>%
  filter(Day == "Monday",
         Time == 9) %>%
  .$IsHDay

p1 <- ggplot() + geom_line(aes(x = 1:length(melb_cent_mon9),
                             y = melb_cent_mon9)) +
      theme_minimal() + xlab("Time (nth Week since 2014)") +
      geom_point(aes(x = 1:length(melb_cent_mon9),
                     y = 0, alpha = monday_hdays),
                 size = 3) + 
      scale_alpha_discrete(name = "Is public holiday") + 
      ylab("Counts at Monday 9am")
p2 <- ggplot() + geom_density(aes(x = melb_cent_mon9)) +
                 theme_minimal() + xlab("Counts at Monday 9am")

grid.arrange(p1, p2, nrow = 1, top = "Monday 9am Counts at Melbourne Central")

```
- can treat each specific hour of the week as a time series
  * find covariates outside the counts data to help predict counts
  * eg. weather data, public events

### Objective: On the fly predictions of pedestrian traffic
- requires efficient data storage

---
# Future steps for final paper
- develop Shiny app wih RStudio to provide on the fly predictions
  * make the predictive models more accessible
  * provide visualisations to present the predictions
  
```{r, cache = TRUE, fig.retina = 4, echo = FALSE, message = FALSE, fig.align= 'center', fig.width=5, fig.height=4, warning=F}
mar13_16_8am <- ped_data[ped_data$Date == "2016-03-13", ] %>%
                filter(Time == 8)

loc$Sensor <- loc$ids

mar13_16_8am %>% left_join(loc) -> mar13_16_8am

bourkest_loc <- loc[loc$ids == "Flinders St Station Underpass",]

  melb_dark <- get_map(location=c(bourkest_loc$lons, bourkest_loc$lats),
                                zoom=16, maptype = "toner-lite")

ggmap(melb_dark) + geom_point(aes(x = lons, y = lats, alpha = Count),
                              colour = magma(8)[6],
                      data = mar13_16_8am, shape = 15, size = 16) +
              scale_alpha_continuous(range = c(.1, .6)) +
              theme_minimal() +
                theme(plot.title = element_text(hjust = 0.5),
                    plot.caption = element_text(hjust = 0.5),
                    axis.text.x = element_blank(),
                    axis.text.y = element_blank(),
                    axis.ticks = element_blank(),
                    legend.position = "right") +
                xlab(NULL) + ylab(NULL) + ggtitle("Pedestrian Traffic around Flinders St \n at 13 Mar 2016, 8AM")

```

  
- explore predictors outside of counts data




